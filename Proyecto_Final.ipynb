{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://construccion.usm.cl/wp-content/uploads/2021/06/logo-usm.png\" width=\"500px\"/>\n",
        "<img src=\"https://fisica.emercom.cl/wp-content/uploads/2023/06/LOGO-TRAN-AZUL-MEDIO.png\" width=\"250px\"/>\n",
        "<H1>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<i>Proyecto Final de Machine Learning para Astronom√≠a</i> </H1>\n",
        "<h1><b><i>‚ö†Ô∏è Asteroides: ¬øPeligroso o No?</i></b></h1>\n",
        "<h4>Clasificaci√≥n de asteroides potencialmente peligrosos usando modelos de Machine Learning</h4>\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "cWj_PYWYnFWc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">\n",
        "<h1></h1>\n",
        "  <img src=\"https://res.cloudinary.com/aenetworks/image/upload/c_fill,ar_2,w_3840,h_1920,g_auto/dpr_auto/f_auto/q_auto:eco/v1/GettyImages-1214628962?_a=BAVAZGID0\">\n",
        "</p>"
      ],
      "metadata": {
        "id": "2T-URSevHGC9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center> Autores <center>\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "| Nombre     | Rol  |\n",
        "|--------------------------------------|-----------------------------------------|\n",
        "| Krishna Sof√≠a Campillay Cort√©s| 202087018-7 |\n",
        "| Francisca Stefania Guti√©rrez Almuna| 202187023-7 |\n"
      ],
      "metadata": {
        "id": "aqui43stnlQV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Selecci√≥n de Datos"
      ],
      "metadata": {
        "id": "IFv779RrIYRg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Origen del dataset:\n",
        "Para este proyecto se utiliz√≥ el siguiente dataset p√∫blico de asteroides disponible en Kaggle:<br>\n",
        "üîó https://www.kaggle.com/datasets/sakhawat18/asteroid-dataset"
      ],
      "metadata": {
        "id": "QAi1FYUBG_XN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rYaStRG3jdh"
      },
      "source": [
        "## Descripci√≥n:\n",
        "Este conjunto de datos re√∫ne informaci√≥n sobre asteroides obtenida por el Laboratorio de Propulsi√≥n a Chorro (JPL) del Instituto Tecnol√≥gico de California con el respaldo de la NASA. Contiene caracter√≠sticas de aproximadamente 245 900 asteroides, incluyendo tama√±o, brillo, par√°metros orbitales y si podr√≠an representar alg√∫n peligro para la Tierra.\n",
        "Estos datos se utilizan con frecuencia en proyectos de machine learning, ya sea para clasificar asteroides (por ejemplo, identificar cu√°les podr√≠an ser potencialmente peligrosos) o para estimar su tama√±o."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6lFto4s45R7"
      },
      "source": [
        "## Explicacion : Nombres de las columnas\n",
        "* id: N√∫mero √∫nico asignado a cada asteroide para su identificaci√≥n.\n",
        "* spkid: Identificador especial utilizado por la NASA para rastrear el asteroide.\n",
        "* full_name: Nombre completo o designaci√≥n del asteroide.\n",
        "* pdes: La etiqueta o n√∫mero principal asignado al asteroide.\n",
        "* name: El nombre com√∫n del asteroide.\n",
        "* prefix: Un prefijo en el nombre del asteroide.\n",
        "* neo: Indica si el asteroide pasa cerca de la Tierra (Y para S√≠, N para No).\n",
        "* pha: Indica si el asteroide podr√≠a ser peligroso para la Tierra (Y para S√≠, N para No).\n",
        "* H: Una medida de que t√°n brillante se ve el asteroide desde la Tierra (mienstras m√°s peque√±o el valor, significa m√°s brillante el objeto).\n",
        "* di√°metro: El tama√±o del asteroide en kil√≥metros.\n",
        "* albedo: Cantidad de luz solar que refleja el asteroide (mientras m√°s alto el valor, significa m√°s reflectante el objeto).\n",
        "* diameter_sigma: La incertidumbre o error en la medici√≥n del tama√±o del asteroide, en kil√≥metros.\n",
        "* orbit_id: Identifica el m√©todo utilizado para calcular la √≥rbita del asteroide.\n",
        "* epoch: La fecha en que se midi√≥ la √≥rbita del asteroide (en un formato especial llamado d√≠as julianos).\n",
        "* √©poca_mjd: La fecha de observaci√≥n en un formato simplificado llamado d√≠as julianos modificados.\n",
        "* √©poca_cal: La fecha de observaci√≥n en formato de calendario.* equinoccio: El sistema de referencia utilizado para las mediciones de la √≥rbita.\n",
        "* e: Cu√°n circular o alargada es la √≥rbita del asteroide (0 = circular, m√°s cerca de 1 = m√°s alargada).\n",
        "* a: La distancia media al Sol en unidades astron√≥micas.\n",
        "* q: La distancia m√°s cercana del asteroide al Sol, en UA\n",
        "* i: El √°ngulo en el que la √≥rbita del asteroide est√° inclinada con respecto al plano del sistema solar, en grados.\n",
        "* om: Un √°ngulo que describe la posici√≥n del asteroide en su √≥rbita, en grados (longitud del nodo ascendente).\n",
        "* w: Otro √°ngulo que describe la orientaci√≥n de la √≥rbita del asteroide, en grados (argumento del perihelio).\n",
        "* ma: Tercer √°ngulo que describe la posici√≥n del asteroide en su √≥rbita, en grados (anomal√≠a media).\n",
        "* ad: La distancia m√°s lejana a la que llega el asteroide del Sol, en UA.\n",
        "* n: La velocidad a la que se mueve el asteroide en su √≥rbita, medida en grados por d√≠a.\n",
        "* tp: La fecha en la que el asteroide est√° m√°s cerca del Sol.\n",
        "* tp_cal: La fecha de m√°xima aproximaci√≥n al Sol en formato de calendario.\n",
        "* per: N√∫mero de d√≠as que tarda el asteroide en completar una √≥rbita alrededor del Sol.\n",
        "* per_y: Periodo orbital en a√±os.\n",
        "* moid: Distancia m√°s cercana a la que llega el asteroide a la Tierra, en UA.\n",
        "* moid_ld: Distancia m√°s cercana a la Tierra, medida en distancias lunares (1 distancia lunar = distancia de la Tierra a la Luna).\n",
        "* sigma_e: La incertidumbre o error en la medici√≥n de la forma de la √≥rbita (e).\n",
        "* sigma_a: La incertidumbre en la distancia media al Sol (a).\n",
        "* sigma_q: La incertidumbre en la distancia m√°s cercana al Sol (q).\n",
        "* sigma_i: La incertidumbre en el √°ngulo de inclinaci√≥n de la √≥rbita (i).\n",
        "* sigma_om: La incertidumbre en el √°ngulo de posici√≥n de la √≥rbita (om).\n",
        "* sigma_w: La incertidumbre en el √°ngulo de orientaci√≥n de la √≥rbita (w).\n",
        "* sigma_ma: La incertidumbre en el √°ngulo de posici√≥n de la √≥rbita (ma).\n",
        "* sigma_ad: La incertidumbre en la distancia m√°s lejana al Sol (ad).\n",
        "* sigma_n: La incertidumbre en la velocidad orbital (n).\n",
        "* sigma_tp: La incertidumbre en la fecha de m√°xima aproximaci√≥n al Sol (tp).\n",
        "* sigma_per: La incertidumbre en el per√≠odo orbital (per).\n",
        "* class: La categor√≠a del asteroide.\n",
        "* rms: El error global en los c√°lculos orbitales (los valores m√°s peque√±os significan mediciones m√°s precisas)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objetivo del An√°lisis\n",
        "La variable objetivo en este an√°lisis es:\n",
        "PHA (Potentially Hazardous Asteroid), columna que indica:<br>\n",
        "YES: El asteroide es potencialmente peligroso <br>\n",
        "NO: El asteroide no es potencialmente peligroso"
      ],
      "metadata": {
        "id": "CDSbiwuIJBhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tipo de Problema\n",
        "Este proyecto corresponde a un problema de:<br>\n",
        "*Clasificaci√≥n*, espec√≠ficamente binaria, ya que la variable objetivo PHA toma los valores Yes o No."
      ],
      "metadata": {
        "id": "PdlPHZ2dJZ2A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploraci√≥n de Datos y Preprocesamiento"
      ],
      "metadata": {
        "id": "RxK2r3FHKg5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Descarga de Dataset"
      ],
      "metadata": {
        "id": "3RIdaMJhKm5I"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPxbLMpYnA3g"
      },
      "source": [
        "Para evitar subir el dataset desde el local optamos por crear un archivo .zip que lo guarda de manera local en el colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXiJBj6OlYO_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1873b1fa-c813-4fec-9af8-858a295aa170"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  182M  100  182M    0     0   119M      0  0:00:01  0:00:01 --:--:--  159M\n"
          ]
        }
      ],
      "source": [
        "!curl -L -o asteroid-dataset.zip https://www.kaggle.com/api/v1/datasets/download/sakhawat18/asteroid-dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWGW1-fJnGSH"
      },
      "source": [
        "Ahora se descomprime el archivo .zip, el cual contiene el datset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCe2IHpbli84",
        "outputId": "b3751037-89d2-4c7a-dcb5-18fbceffcb01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ./asteroid-dataset.zip\n",
            "replace dataset.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "  inflating: dataset.csv             \n"
          ]
        }
      ],
      "source": [
        "!unzip ./asteroid-dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importaci√≥n de Librer√≠as"
      ],
      "metadata": {
        "id": "u1B-zhxQLfUG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se utilizan librer√≠as que hemos utilizado en el ramo de machine learning para la astrof√≠sica, estas librer√≠as permitir√°n realizar: exploraci√≥n, limpieza, transformaci√≥n de datos, balanceo de clases, construcci√≥n de modelos y evaluaci√≥n de su desempe√±o."
      ],
      "metadata": {
        "id": "wQcKapAKMBI5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUUXAYYZEiJE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Detecci√≥n de Outliers\n",
        "from scipy import stats\n",
        "\n",
        "# Codificaci√≥n y transformaci√≥n de datos\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Dividir datos, entrenar modelos y evaluar rendimiento\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor, RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "from imblearn.combine import SMOTETomek\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conociendo el dataset"
      ],
      "metadata": {
        "id": "4utkbOFnOQPp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcEy-z0OGShH"
      },
      "source": [
        "Cargamos y visualizamos los datos de nuestro dataset. Veamos las primeras filas y las √∫ltimas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DY9FjuMfGBpd"
      },
      "outputs": [],
      "source": [
        "datos = pd.read_csv(\"dataset.csv\")\n",
        "datos.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datos.tail()"
      ],
      "metadata": {
        "id": "B2cruNivPCNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkcLMhvCHUKX"
      },
      "source": [
        "Conozcamos la cantidad de filas y columnas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLz2dUDQHADd"
      },
      "outputs": [],
      "source": [
        "datos.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos un resumen del Dataset viendo cantidad de filas, columnas, valores no nulos y tipos de datos:"
      ],
      "metadata": {
        "id": "NRIidd4CPYnt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVE6a356Hc_9"
      },
      "outputs": [],
      "source": [
        "datos.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conozcamos cu√°ntos valores faltantes (Nan) tiene cada columna del Dataset:"
      ],
      "metadata": {
        "id": "TjzX0uREP13B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zI40kvOFIiIe"
      },
      "outputs": [],
      "source": [
        "datos.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Redundancia y Valores faltantes"
      ],
      "metadata": {
        "id": "rXMyUO24QN4G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eliminemos varias columnas ya que no aportan valor al modelo:\n",
        "1. Identificadores y nombres como id, spkid, full_name, pdes, name, prefix, que no contienen informaci√≥n √∫til para predecir PHa, en su mayor√≠a son datos cualitativos que ayudan a identificarlos.\n",
        "2. Columnas redundantes como moid_ld y per_y, cuyo contenido ya est√° representado en otras variables.\n",
        "3. Fechas y par√°metros orbitales avanzados como epoch, epoch_mjd, epoch_cal, equinox, tp, tp_cal, que no son relevantes para el objetivo.\n",
        "4. Columnas de incertidumbre como sigma_e, sigma_a, sigma_q, etc., que describen errores de medici√≥n y no caracter√≠sticas del asteroide.\n",
        "5. Metricas de ajuste como rms, que no aportan al proceso predictivo."
      ],
      "metadata": {
        "id": "L8ENBEylQUb8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y56cS6RAiM73"
      },
      "outputs": [],
      "source": [
        "columnas_no_utiles= ['id', 'spkid', 'full_name', 'pdes', 'name', 'prefix', 'diameter_sigma', 'orbit_id', 'epoch', 'epoch_mjd', 'epoch_cal','equinox', 'tp', 'tp_cal','sigma_e', 'sigma_a', 'sigma_q','sigma_i', 'sigma_om', 'sigma_w', 'sigma_ma', 'sigma_ad', 'sigma_n','sigma_tp',\n",
        "                     'sigma_per', 'rms','moid_ld', 'per_y']\n",
        "\n",
        "# Quitamos las columnas del dataframe original que no nos sirven como ya se justific√≥\n",
        "datos.drop(columns=columnas_no_utiles, inplace=True)\n",
        "datos.info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za7mNYqvL9UA"
      },
      "source": [
        "Manejemos los valores faltantes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLmEKcwML7DE"
      },
      "outputs": [],
      "source": [
        "# Contar los valores faltantes en cada fila\n",
        "valores_faltantes = datos.isna().sum(axis=1)\n",
        "\n",
        "# Identifica las filas con dos o m√°s valores faltantes\n",
        "n_filas_eliminar = (valores_faltantes >= 2).sum()\n",
        "print(f\"N√∫mero total de filas con 2 o m√°s valores faltantes: {n_filas_eliminar}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hW6ItP1FPYVh"
      },
      "outputs": [],
      "source": [
        "# Contar las ocurrencias de cada valor faltante\n",
        "distribucion_faltante = valores_faltantes[valores_faltantes >= 2].value_counts().sort_index()\n",
        "print(\"Distribuci√≥n de valores faltantes por fila:\")\n",
        "print(distribucion_faltante)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dez8d2n2R_tq"
      },
      "outputs": [],
      "source": [
        "# Calcular porcentajes de valores faltantes\n",
        "porcentajes_faltantes = datos.isnull().mean() * 100\n",
        "\n",
        "# Categorizar columnas seg√∫n rangos de porcentaje de valores faltantes\n",
        "rango = {\"0-10%\":[], \"10-20%\":[],\"20-30%\":[],\"30-40%\":[],\"50-50%\":[],\"50-60%\":[],\"60-70%\":[],\"70-80%\":[],\"80-90%\":[],\"90-100%\":[]}\n",
        "\n",
        "for i, perc in porcentajes_faltantes.items():\n",
        "    if 0 <= perc < 10:\n",
        "        rango[\"0-10%\"].append(i)\n",
        "    elif 10 <= perc < 20:\n",
        "        rango[\"10-20%\"].append(i)\n",
        "    elif 20 <= perc < 30:\n",
        "        rango[\"20-30%\"].append(i)\n",
        "    elif 30 <= perc < 40:\n",
        "        rango[\"30-40%\"].append(i)\n",
        "    elif 40 <= perc < 50:\n",
        "       rango[\"40-50%\"].append(i)\n",
        "    elif 50 <= perc < 60:\n",
        "        rango[\"50-60%\"].append(i)\n",
        "    elif 60 <= perc < 70:\n",
        "        rango[\"60-70%\"].append(i)\n",
        "    elif 70 <= perc < 80:\n",
        "        rango[\"70-80%\"].append(i)\n",
        "    elif 80 <= perc < 90:\n",
        "        rango[\"80-90%\"].append(i)\n",
        "    elif 90 <= perc <= 100:\n",
        "        rango[\"90-100%\"].append(i)\n",
        "\n",
        "# mostramos los resultados\n",
        "for rangos, i in rango.items():\n",
        "    print(f\"Columnas con valores faltantes en el rango {rangos}: {i}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-8VUklhWawq"
      },
      "outputs": [],
      "source": [
        "# Gestionar columnas con pocos valores faltantes (0-10%)\n",
        "\n",
        "#utilizamos la moda en las columnas\n",
        "datos['neo'].fillna(datos['neo'].mode()[0], inplace=True)\n",
        "datos['pha'].fillna(datos['pha'].mode()[0], inplace=True)\n",
        "\n",
        "# utilizamos la mediana en la columnas\n",
        "n_columnas = ['H', 'ma', 'ad', 'per', 'moid']\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "datos[n_columnas] = imputer.fit_transform(datos[n_columnas])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3znWRY3YuBO"
      },
      "outputs": [],
      "source": [
        "# creamos una copia para la clasificaci√≥n\n",
        "datos_clasificacion = datos.copy()\n",
        "\n",
        "# Eliminar filas con diameter\n",
        "datos_clasificacion.dropna(subset=['diameter'], inplace=True)\n",
        "\n",
        "datos_clasificacion['albedo'].fillna(datos_clasificacion['albedo'].median(), inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1S9xkdNDbumq"
      },
      "outputs": [],
      "source": [
        "# Contar los valores faltantes en cada fila\n",
        "valores_faltantes = datos_clasificacion.isna().sum(axis=1)\n",
        "\n",
        "# Identifica las filas con dos o m√°s valores faltantes\n",
        "n_filas_eliminar = (valores_faltantes >= 2).sum()\n",
        "print(f\"\\n N√∫mero total de filas con 2 o m√°s valores faltantes: {n_filas_eliminar}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "457J4V2Kb-YJ"
      },
      "outputs": [],
      "source": [
        "# Contar las ocurrencias de cada valor faltante\n",
        "distribucion_faltante = valores_faltantes[valores_faltantes >= 2].value_counts().sort_index()\n",
        "print(\"Distribuci√≥n de valores faltantes por fila:\")\n",
        "print(distribucion_faltante)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S28Jx35KcTJq"
      },
      "outputs": [],
      "source": [
        "datos_clasificacion.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4qsw-mUcdmL"
      },
      "outputs": [],
      "source": [
        "datos_clasificacion.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZEXHOg0cjU4"
      },
      "outputs": [],
      "source": [
        "datos_clasificacion.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "he7RM8YXcwvN"
      },
      "outputs": [],
      "source": [
        "datos_clasificacion.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtcXjrIFQpB5"
      },
      "source": [
        "## Outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Revisar los outliers es importante porque pueden distorsionar los resultados, afectar las m√©tricas y conducir a conclusiones incorrectas. Detectarlos permite decidir si deben corregirse o eliminarse, asegurando un an√°lisis m√°s preciso y confiable, ahora veremos como interfieren en nuestro proyecto"
      ],
      "metadata": {
        "id": "LQCAO5k05oyX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLeOEdBXQoBF"
      },
      "outputs": [],
      "source": [
        "outlier_columnas = []\n",
        "\n",
        "for columnas in datos_clasificacion.select_dtypes(include=['number']).columns:\n",
        "    Q1 = datos_clasificacion[columnas].quantile(0.25)\n",
        "    Q3 = datos_clasificacion[columnas].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    limite_inferior = Q1 - 1.5 * IQR\n",
        "    limite_superior = Q3 + 1.5 * IQR\n",
        "\n",
        "    outliers = datos_clasificacion[(datos_clasificacion[columnas] < limite_inferior) | (datos_clasificacion[columnas] > limite_superior)]\n",
        "\n",
        "    if not outliers.empty:\n",
        "        outlier_columnas.append(columnas)\n",
        "        print(f\"la Columna '{columnas}' tiene {len(outliers)} outliers.\")\n",
        "\n",
        "print(\"Columnas con outliers:\", outlier_columnas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpVtOl0NddlN"
      },
      "outputs": [],
      "source": [
        "#  Z-Score\n",
        "outlier_zscore = []\n",
        "\n",
        "for columna in datos_clasificacion.select_dtypes(include=['number']).columns:\n",
        "    z_scores = stats.zscore(datos_clasificacion[columna].dropna())  # Eliminar los valores NaN para evitar errores\n",
        "    outliers = (abs(z_scores) > 3).sum()\n",
        "\n",
        "    if outliers > 0:\n",
        "        outlier_zscore.append(columna)\n",
        "        print(f\"Columna'{columna}' que tienen {outliers} outliers.\")\n",
        "\n",
        "print(\"Columnas con outliers (Z-score):\", outlier_zscore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRHO9WfNe6yJ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# solo columnas num√©ricas que no est√©n vac√≠as\n",
        "num_columnas = datos_clasificacion.select_dtypes(include=['number']).columns\n",
        "num_columnas = [col for col in num_columnas if not datos_clasificacion[col].isnull().all()]\n",
        "\n",
        "# Definir la disposici√≥n de la cuadr√≠cula\n",
        "n_columnas = 4  # Aumentamos columnas para hacer cada gr√°fico m√°s peque√±o\n",
        "n_filas = -(-len(num_columnas) // n_columnas)\n",
        "\n",
        "# Reducimos el tama√±o total de la figura\n",
        "fig, axes = plt.subplots(nrows=n_filas, ncols=n_columnas, figsize=(14, 3 * n_filas))\n",
        "\n",
        "# Aplanamos\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Boxplots\n",
        "for i, columna in enumerate(num_columnas):\n",
        "    sns.boxplot(y=datos_clasificacion[columna], ax=axes[i])\n",
        "    axes[i].set_title(f'{columna}', fontsize=9)\n",
        "    axes[i].tick_params(labelsize=8)\n",
        "\n",
        "# Ocultar subplots vac√≠os\n",
        "for i in range(len(num_columnas), len(axes)):\n",
        "    fig.delaxes(axes[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el contexto astron√≥mico, los outliers no siempre representan errores o ruido en los datos, sino que pueden corresponder a fen√≥menos reales y cient√≠ficamente relevantes y como nuestro an√°lisis busca entender la diversidad y caracter√≠sticas de los asteroides, conservar estos valores resulta esencial para no perder informaci√≥n valiosa que podr√≠a llevar a descubrimientos o interpretaciones significativas.\n",
        "\n",
        "En particular, para la clasificaci√≥n de Pha o no-Pha se consideran dos criterios oficiales de la NASA:\n",
        "\n",
        "* MOID‚â§ 0.05UA ‚Üí indica que la √≥rbita del asteroide se acerca lo suficiente a la Tierra.\n",
        "* Magnitud absoluta H‚â§ 22 ‚Üí corresponde aproximadamente a asteroides de ‚â•140m de di√°metro, lo suficientemente grandes como para causar da√±os significativos en caso de impacto.\n",
        "\n",
        "Los outliers de H y MOID pueden ser muy relevantes en este contexto:\n",
        "\n",
        "* Los H bajos representan asteroides grandes, que cumplen con el criterio de tama√±o para ser PHA.\n",
        "* Los MOID bajos indican proximidad a la √≥rbita de la Tierra, otro criterio esencial para la clasificaci√≥n.\n",
        "* Los H extremadamente altos (por ejemplo en H sobre 25) podr√≠an corresponder a asteroides muy peque√±os o errores de medici√≥n y, si se considerara, podr√≠an filtrarse selectivamente, pero no de manera autom√°tica.\n",
        "\n",
        " Por esta raz√≥n, en la etapa de limpieza de datos no se aplic√≥ una eliminaci√≥n de outliers."
      ],
      "metadata": {
        "id": "C1KMArh1ZfQl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Antes de: Otro Preprocesamiento"
      ],
      "metadata": {
        "id": "SdJS6pFVbLGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora realizaremos un preprocesamiento de datos para realizar un analisis mas profundo de nuestras variables\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JYMmcMcq5_HP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTSUDuGrgoQq"
      },
      "outputs": [],
      "source": [
        "# Preparamos los datos convirtiendo las columnas categ√≥ricas a valores num√©ricos\n",
        "datos_n_clasificacion = datos_clasificacion.copy()\n",
        "\n",
        "# Transformamos 'neo' y 'pha' desde Y/N a 1/0 para que el modelo pueda interpretarlos\n",
        "datos_n_clasificacion['neo'] = datos_n_clasificacion['neo'].map({'Y': 1, 'N': 0})\n",
        "datos_n_clasificacion['pha'] = datos_n_clasificacion['pha'].map({'Y': 1, 'N': 0})\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizamos como queda la nueva tranformacion de los datos"
      ],
      "metadata": {
        "id": "lYUnKemH6XS5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ex8NHg_gjkwF"
      },
      "outputs": [],
      "source": [
        "datos_n_clasificacion.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqw87clMjvwU"
      },
      "outputs": [],
      "source": [
        "datos_n_clasificacion.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Gr√°ficos KDE y de Conteo:\n"
      ],
      "metadata": {
        "id": "6MBSPKkcj2Hk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7km2KF_jlhIz"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "# Columnas num√©ricas\n",
        "columnas = ['H', 'diameter', 'albedo', 'e',\n",
        "            'a', 'q', 'i',\n",
        "            'om', 'w', 'ma',\n",
        "            'ad', 'n', 'per',\n",
        "            'moid']\n",
        "\n",
        "# Columnas categ√≥ricas\n",
        "cat_columnas = ['neo', 'pha', 'class']\n",
        "\n",
        "# Gr√°ficos KDE para columnas num√©ricas\n",
        "n_columnas = 3\n",
        "n_filas = math.ceil(len(columnas) / n_columnas)\n",
        "\n",
        "fig, axes = plt.subplots(nrows=n_filas, ncols=n_columnas, figsize=(7*n_columnas, 3*n_filas))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, columna in enumerate(columnas):\n",
        "    sns.kdeplot(data=datos_n_clasificacion[columna], ax=axes[i], fill=True)\n",
        "    axes[i].set_title(f'Gr√°fico KDE de {columna}')\n",
        "\n",
        "# Eliminar ejes vac√≠os si los hay\n",
        "for i in range(len(columnas), len(axes)):\n",
        "    fig.delaxes(axes[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* La mayor√≠a de los asteroides son peque√±os, poco brillantes, lo que indica superficies oscuras que reflejan poca luz. Sus √≥rbitas suelen estar cerca de la Tierra, con valores de a, q y ad concentrados en rangos bajos, y con inclinaciones peque√±as que los mantienen cerca del plano del Sistema Solar.\n",
        "\n",
        "* La excentricidad es moderada, lo que sugiere √≥rbitas algo alargadas, mientras que las velocidades orbitales (n) son muy similares entre s√≠. En conjunto, los asteroides del conjunto de datos forman una poblaci√≥n compacta y bastante homog√©nea en tama√±o y comportamiento orbital."
      ],
      "metadata": {
        "id": "PghZPP1OK5wX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Gr√°ficos de recuento para columnas categ√≥ricas\n",
        "n_columnas_cat = 3\n",
        "n_filas_cat = math.ceil(len(cat_columnas) / n_columnas_cat)\n",
        "\n",
        "fig, axes = plt.subplots(nrows=n_filas_cat, ncols=n_columnas_cat, figsize=(5*n_columnas_cat, 4*n_filas_cat))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, columna in enumerate(cat_columnas):\n",
        "    sns.countplot(data=datos_n_clasificacion, x=columna, ax=axes[i])\n",
        "    axes[i].set_title(f'Gr√°fico de conteo de {columna}')\n",
        "    axes[i].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Eliminar ejes vac√≠os si los hay\n",
        "for i in range(len(cat_columnas), len(axes)):\n",
        "    fig.delaxes(axes[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B6nKc3bL8dkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* La gran mayor√≠a de los asteroides del conjunto de datos no son NEO, es decir, no se acercan a la √≥rbita terrestre, mientras que solo una fracci√≥n muy peque√±a aparece clasificada como NEO = 1. Lo que indica que los objetos que realmente pasan cerca de la Tierra representan una minor√≠a dentro del cat√°logo total, reflejando que los asteroides con trayectorias cercanas a nuestro planeta son mucho menos frecuentes que aquellos que permanecen en regiones m√°s estables del Sistema Solar.\n",
        "\n",
        "\n",
        "* La distribuci√≥n de pha muestra que pr√°cticamente todos los asteroides del conjunto de datos est√°n clasificados como no peligrosos (pha = 0), mientras que solo una fracci√≥n muy peque√±a aparece como potencialmente peligrosa (pha = 1). Esto confirma que los objetos que representan un riesgo real para la Tierra son extremadamente raros dentro de la poblaci√≥n total, en concordancia con los an√°lisis que indican que la gran mayor√≠a de los asteroides no cruza la √≥rbita terrestre de manera riesgosa.\n",
        "\n",
        "\n",
        "* La categor√≠a MBA (Main Belt Asteroids) domina por completo el conjunto, con m√°s de 120.000 objetos.\n",
        "Esto significa que la mayor√≠a de los asteroides provienen del cintur√≥n entre Marte y J√∫piter.\n",
        "\n",
        "* Las dem√°s clases (OMB, MCA, AMO, IMB, etc.) aparecen con cantidades muy peque√±as en comparaci√≥n.\n",
        "\n",
        "* Las clases asociadas a objetos cercanos a la Tierra (como APO, ATE, AMO) tambi√©n son minoritarias, lo cual es coherente con los conteos de NEO y PHA."
      ],
      "metadata": {
        "id": "W96huX1-MRXN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correlaciones"
      ],
      "metadata": {
        "id": "0hrH5sqxmos0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usaremos un Heatmap para visualizar las correlaciones"
      ],
      "metadata": {
        "id": "jkH9-yiPmsz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "matrix = datos_n_clasificacion[num_cols+[\"pha\",\"neo\"]].corr()\n",
        "sns.heatmap(matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt='.2f')\n",
        "plt.title('heatmap de correlaci√≥n para columnas num√©ricas')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SScw7iAimoAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlaciones altas:\n",
        "\n",
        "* a (Distancia √ìrbita) y ad (Distancia Sol M√°s Lejana) (0,99)\n",
        "* a (Distancia √ìrbita) y per (Per√≠odo √ìrbita D√≠as) (0,94)\n",
        "* q (Distancia Sol M√°s Cercana) y moid( Distancia Tierra M√°s Cercana) (1).\n",
        "* ad (Distancia Sol M√°s Lejana) y per(Per√≠odo √ìrbita D√≠as) (0,97)\n"
      ],
      "metadata": {
        "id": "iXDLh1ARmzJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Profundicemos: Correlaci√≥n de las variables con la Variable Objetivo"
      ],
      "metadata": {
        "id": "CeoFivJvqPBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tomemos las variables con mayor valor de correlaci√≥n con la variable pha, las cuales ser√°n las predictoras cuando entrenemos el modelo. Las variables son: H, moid, e, q y n"
      ],
      "metadata": {
        "id": "TYkGbnhIql3d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9Rjd3aErp5V"
      },
      "outputs": [],
      "source": [
        "# Variables num√©ricas vs PHA\n",
        "num_pares = [('pha', 'H'), ('pha', 'moid'), ('pha', 'e'), ('pha', 'q'), ('pha', 'n')]\n",
        "\n",
        "graf_tipos = [\"box\", \"box\", \"bar\", \"bar\", \"kde\"]\n",
        "\n",
        "n_columnas = 2\n",
        "n_filas = math.ceil(len(num_pares) / n_filas)\n",
        "\n",
        "fig, axes = plt.subplots(n_filas, n_filas, figsize=(5*n_filas, 3.5*n_filas))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Graficamos cada par seg√∫n el tipo indicado\n",
        "for i, (x_col, y_col) in enumerate(num_pares):\n",
        "    ax = axes[i]\n",
        "    tipo = graf_tipos[i]\n",
        "\n",
        "    if tipo == \"box\":\n",
        "        sns.boxplot(data=datos_n_clasificacion, x=x_col, y=y_col, ax=ax)\n",
        "        ax.set_title(f\"Boxplot: {y_col} seg√∫n {x_col}\")\n",
        "\n",
        "    elif tipo == \"bar\":\n",
        "        sns.barplot(data=datos_n_clasificacion, x=x_col, y=y_col, ax=ax, errorbar=\"se\")\n",
        "        ax.set_title(f\"Media + Error: {y_col} seg√∫n {x_col}\")\n",
        "\n",
        "    elif tipo == \"kde\":\n",
        "        sns.kdeplot(data=datos_n_clasificacion, x=y_col, hue=x_col, ax=ax, fill=True, alpha=0.5)\n",
        "        ax.set_title(f\"KDE: Distribuci√≥n de {y_col} seg√∫n {x_col}\")\n",
        "\n",
        "# Quitamos subplots vac√≠os si sobran\n",
        "for i in range(len(num_pares), len(axes)):\n",
        "    fig.delaxes(axes[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*De los siguientes graficos podemos concluir que*\n",
        "* Los asteroides peligrosos (pha = 1) tienen valores de H m√°s altos, lo que significa que se ven menos brillantes y suelen ser algo m√°s peque√±os. En cambio, los no peligrosos (pha = 0) muestran m√°s brillo y mucha mayor variabilidad, con muchos outliers, indicando tama√±os y caracter√≠sticas muy diversas.\n",
        "* Los asteroides peligrosos (pha = 1) tienen una distancia m√≠nima a la Tierra (moid) mucho menor, lo que confirma que pasan realmente cerca. Los no peligrosos (pha = 0) tienen moid mucho m√°s grande y disperso, lo que indica que no representan riesgo porque sus √≥rbitas quedan lejos de la Tierra.\n",
        "\n",
        "* La excentricidad (e) es mucho mayor en los asteroides peligrosos (pha = 1).\n",
        "Esto significa que sus √≥rbitas son m√°s alargadas, mientras que los asteroides no peligrosos (pha = 0) tienen √≥rbitas m√°s circulares. Una √≥rbita m√°s alargada puede facilitar que el asteroide cruce la √≥rbita de la Tierra, lo cual explica por qu√© aparecen clasificados como potencialmente peligrosos.\n",
        "\n",
        "* La distancia m√≠nima al Sol (q) es mucho menor para los asteroides peligrosos (pha = 1).\n",
        "Esto indica que estos asteroides se acercan m√°s al Sol en su √≥rbita, lo que suele estar asociado a trayectorias que cruzan o se acercan m√°s a la √≥rbita de la Tierra, aumentando el riesgo. Por el contrario, los asteroides no peligrosos (pha = 0) tienen valores de q mayores, lo que sugiere √≥rbitas m√°s alejadas y menos probabilidad de intersecci√≥n con la Tierra.\n",
        "\n",
        "* Los asteroides no peligrosos (pha = 0) muestran una distribuci√≥n muy concentrada de n, lo que indica velocidades orbitales similares. En cambio, los peligrosos (pha = 1) son pocos y su distribuci√≥n es baja y dispersa, por lo que la velocidad angular no es un factor que diferencie el riesgo: lo que importa es la trayectoria, no la velocidad"
      ],
      "metadata": {
        "id": "iPEP7O-4A-Vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NEO vs PHA matriz de confusion\n",
        "matriz_conf = pd.crosstab(datos_n_clasificacion['neo'], datos_n_clasificacion['pha'])\n",
        "\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(matriz_conf, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('NEO vs PHA')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UzGsYNrz9uCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* La mayor√≠a de los asteroides no son NEO y tampoco son peligrosos (135.360).\n",
        "\n",
        "* Hay 628 asteroides NEO que no son peligrosos, es decir, pasan cerca de la Tierra pero no representan riesgo.\n",
        "\n",
        "* Solo 221 asteroides son NEO y adem√°s peligrosos (PHA).\n",
        "\n",
        "* No existe ning√∫n asteroide que sea PHA sin ser NEO, lo cual tiene sentido: solo un objeto que se acerca a la Tierra puede considerarse potencialmente peligroso."
      ],
      "metadata": {
        "id": "s7C8-il3FNIX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQOZ5Dhq9-Ek"
      },
      "source": [
        "#  Modelos y Entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Justificaci√≥n de elecci√≥n de Modelos\n",
        "\n",
        "Antes de empezar a entrenar cualquier modelo, a partir del an√°lisis f√≠sico y estad√≠stico del dataset anterior se puede evaluar qu√© tipo de algoritmo se adapta mejor a la estructura de los datos y al fen√≥meno que buscamos modelar, por ende decidimos trabajar con Random Forest y Gradient Boosting Classifier.\n",
        "\n",
        "\n",
        "```\n",
        "# Ya est√°n importadas\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "5fPToxV-pZsr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest: Nos permite capturar relaciones altamente no lineales entre los par√°metros orbitales sin asumir formas funcionales espec√≠ficas, adem√°s la clasificaci√≥n PHA depende de umbrales f√≠sicos y un modelo basado en √°rboles reproduce de manera natural estas reglas de decisi√≥n. Adem√°s, dado que observamos correlaciones fuertes entre varios elementos orbitales, como a, q, ad y per, consideramos que un algoritmo que tolere multicolinealidad es fundamental, y RF cumple este requisito.\n",
        "\n",
        "Gradient Boosting Classifier: Ofrece una capacidad m√°s refinada para identificar patrones complejos en los casos l√≠mite, donde la distinci√≥n entre PHA y no-PHA no es tan evidente. GBC construye √°rboles de manera secuencial, corrigiendo iterativamente los errores previos, esto le permite detectar detalles sutiles en regiones donde los asteroides tienen par√°metros muy cercanos a los umbrales f√≠sicos conocidos.\n",
        "\n",
        "*Observaci√≥n:* Desde un inicio los modelos lineales fueron descartados, ya que el an√°lisis del dataset indican distribuciones altamente asim√©tricas, outliers f√≠sicos y relaciones no lineales. Adem√°s, la regla de decisi√≥n que define un PHA no es aproximable mediante una separaci√≥n lineal."
      ],
      "metadata": {
        "id": "oXEA4yp7s-6S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ¬øQu√© propiedades del dataset influyeron en nuestra elecci√≥n?\n",
        "\n",
        "Correlaciones extremadamente altas entre par√°metros orbitales: Los diagramas de correlaci√≥n muestran que variables como a, ad, per y q est√°n estrechamente relacionadas y esta redundancia favorece modelos que puedan ignorar autom√°ticamente variables duplicadas o casi duplicadas, como es el caso de los algoritmos basados en √°rboles.\n",
        "\n",
        "La etiqueta PHA depende directamente de par√°metros f√≠sicos concretos: MOID es una medida expl√≠cita de la distancia m√≠nima entre la √≥rbita del asteroide y la √≥rbita terrestre, mientras que H est√° relacionada con el tama√±o del cuerpo. Ambos factores son determinantes f√≠sicos de peligrosidad.\n",
        "\n",
        "El conjunto de datos est√° fuertemente desbalanceado: Detectamos que la cantidad de asteroides clasificados como PHA es muy peque√±a en comparaci√≥n con el total y eso obliga a utilizar modelos que permitan ajustar pesos de clase, corregir desbalances y posteriormente manipular el umbral de decisi√≥n para aumentar el recall sin sacrificar interpretabilidad.\n",
        "\n",
        "El volumen del dataset es suficientemente grande: Contar con m√°s de 130.000 observaciones filtradas despu√©s del preprocesamiento nos permite emplear modelos ensemble sin riesgo de sobreajuste inmediato y aprovechar su capacidad para descubrir reglas de decisi√≥n complejas.\n",
        "\n",
        "Variables como el di√°metro, el per√≠odo orbital (per) y la distancia m√≠nima a la Tierra (moid) muestran distribuciones altamente asim√©tricas, con colas largas hacia la derecha. Este comportamiento sugiere la existencia de pocos valores extremos que pueden desestabilizar modelos sensibles a outliers, favoreciendo algoritmos robustos como √°rboles o modelos basados en medianas\n",
        "\n",
        "Los conteos y la matriz de confusion confirman que todos los asteroides PHA pertenecen al subconjunto de NEO, es decir, objetos con √≥rbitas que cruzan o se acercan a la √≥rbita terrestre. Este patr√≥n estructural implica que la mayor√≠a de los asteroides del cintur√≥n principal (MBA) dif√≠cilmente aportan informaci√≥n √∫til para clasificar peligrosidad, lo que explica por qu√© las clases est√°n fuertemente desbalanceadas y la etiqueta PHA depende de un subconjunto muy espec√≠fico del dataset.\n",
        "\n",
        "Las variables angulares (om, w, ma) presentan distribuciones casi uniformes, lo que indica que sus orientaciones orbitales son aleatorias dentro de la muestra y no contienen estructura predictiva importante para determinar peligrosidad. Esto permite descartarlas sin p√©rdida de informaci√≥n relevante.\n"
      ],
      "metadata": {
        "id": "4n0iMOlFp3ji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hiperpar√°metros que consideramos necesarios ajustar\n",
        "\n",
        "Antes de pasar al entrenamiento, identificamos los hiperpar√°metros m√°s relevantes que deben optimizarse para extraer el m√°ximo rendimiento de los modelos, considerando especialmente la prioridad cient√≠fica de maximizar el recall de la clase PHA.\n",
        "\n",
        "**Random Forest**\n",
        "\n",
        "1. *n_estimators:*\n",
        "Ajustaremos la cantidad de √°rboles (200, 500, 1000) para obtener un equilibrio entre estabilidad y costo computacional.\n",
        "\n",
        "2. *max_depth:*\n",
        "Regular√° la complejidad del modelo para evitar que el bosque aprenda patrones demasiado espec√≠ficos derivados de casos aislados. Consideramos profundidades entre 6, 12 y sin l√≠mite.\n",
        "\n",
        "3. *min_samples_leaf:*\n",
        "Impedir√° que el modelo genere reglas basadas en muy pocas observaciones. Usaremos valores de 1, 2, 5 y 10.\n",
        "\n",
        "4. *max_features:*\n",
        "Exploraremos \"sqrt\", \"log2\", 0.3 y None para manejar adecuadamente la redundancia entre variables orbitales.\n",
        "\n",
        "5. *class_weight:*\n",
        "Utilizaremos \"balanced\" para enfrentar el fuerte desbalance de clases observado.\n",
        "\n",
        "\n",
        "**Gradient Boosting Classifier**\n",
        "\n",
        "6. *learning_rate:*\n",
        "Es el par√°metro m√°s cr√≠tico en GBC. Probamos valores cercanos a 0.01, 0.05 y 0.1 para ajustar el nivel de correcci√≥n incremental.\n",
        "\n",
        "7. *n_estimators:*\n",
        "Ajustado conjuntamente con el learning_rate, probaremos entre 200 y 1000 √°rboles.\n",
        "\n",
        "8. *max_depth:*\n",
        "Usaremos √°rboles base poco profundos (3, 4 y 6) para evitar sobreajuste.\n",
        "\n",
        "9. *subsample:*\n",
        "Evaluaremos valores entre 0.6 y 0.8 para introducir aleatoriedad y mejorar la generalizaci√≥n.\n",
        "\n",
        "10. *min_samples_leaf y min_samples_split:*\n",
        "Nos permitir√°n evitar que el modelo memorice casos aislados o poco representativos.\n",
        "\n",
        "11. *Early stopping:*\n",
        "Activaremos validaci√≥n interna para detener el entrenamiento cuando el modelo deje de mejorar, controlando de esta forma la variabilidad."
      ],
      "metadata": {
        "id": "oPaIHY-tqEa_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgXt3-cIRKRN"
      },
      "source": [
        "## Fase de preparaci√≥n de los datos para entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znaoTuM59-dJ"
      },
      "outputs": [],
      "source": [
        "# Descartamos pha, para apartarlo como Target del modelo\n",
        "X = datos_n_clasificacion.drop(columns=[\"pha\",\"class\",\"i\",\"om\",\"w\",\"albedo\"])\n",
        "y = datos_n_clasificacion[\"pha\"]\n",
        "\n",
        "# Dividimos el conjunto en un conjunto de entrenamiento y otro de prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Verificar\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## M√©tricas Evaluadoras:\n",
        "Para evaluar la capacidad de los modelos para predecir si un asteroide corresponde a un PHA, emplearemos m√©tricas que permitan medir tanto la calidad global de las predicciones como qu√© tan bien identifica la clase Pha, donde Accuracy entrega una visi√≥n general del porcentaje total de aciertos del modelo, mientras que la Precision indica qu√© tan confiable es la predicci√≥n cuando el modelo clasifica un asteroide como peligroso, algo crucial para evitar falsas alarmas y a  su vez, la Recall mide cu√°ntos asteroides realmente peligrosos logra detectar el modelo, lo cual es especialmente importante dado el riesgo asociado a no identificar un PHA y se utilizar√° AUC, que eval√∫a la capacidad del modelo para separar correctamente asteroides peligrosos de no peligrosos considerando sus probabilidades, permitiendo ver qu√© tan bien discrimina m√°s all√° de un umbral fijo.\n",
        "\n",
        "Adem√°s evaluaremos el comportamiento del modelo en t√©rminos de Bias y Variance mediante los an√°lisis de entrenamiento y prueba."
      ],
      "metadata": {
        "id": "w-MLSHQx66eA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc2p81Qt7P4D"
      },
      "source": [
        "## Fase de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChD1MXrdcB6T"
      },
      "outputs": [],
      "source": [
        "# Modelos\n",
        "modelos = {\n",
        "    \"RF\": RandomForestClassifier(),\n",
        "    \"GBC\": GradientBoostingClassifier()\n",
        "}\n",
        "\n",
        "# Diccionario para guardar m√©tricas\n",
        "resultados = {\n",
        "    \"model\": [],\n",
        "    \"train_accuracy\": [], \"train_precision\": [], \"train_recall\": [], \"train_auc\": [],\n",
        "    \"test_accuracy\": [], \"test_precision\": [], \"test_recall\": [], \"test_auc\": [],\n",
        "    \"bias_gap\": [], \"variance_gap\": []\n",
        "}\n",
        "\n",
        "# Entrenar y evaluar cada modelo\n",
        "for nombre, modelamiento in modelos.items():\n",
        "    modelamiento.fit(X_train, y_train)\n",
        "\n",
        "    # Predicciones duras\n",
        "    y_train_pred = modelamiento.predict(X_train)\n",
        "    y_test_pred = modelamiento.predict(X_test)\n",
        "\n",
        "    # Predicciones probabil√≠sticas\n",
        "    y_train_prob = modelamiento.predict_proba(X_train)[:, 1]\n",
        "    y_test_prob = modelamiento.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # M√©tricas de entrenamiento\n",
        "    train_acc = accuracy_score(y_train, y_train_pred)\n",
        "    train_prec = precision_score(y_train, y_train_pred)\n",
        "    train_rec = recall_score(y_train, y_train_pred)\n",
        "    train_auc = roc_auc_score(y_train, y_train_prob)\n",
        "\n",
        "    # M√©tricas de prueba\n",
        "    test_acc = accuracy_score(y_test, y_test_pred)\n",
        "    test_prec = precision_score(y_test, y_test_pred)\n",
        "    test_rec = recall_score(y_test, y_test_pred)\n",
        "    test_auc = roc_auc_score(y_test, y_test_prob)\n",
        "\n",
        "    # Diagn√≥stico de bias y varianza\n",
        "    bias_gap = abs(1 - train_acc)                 # Si es alto ‚Üí mucho bias\n",
        "    variance_gap = abs(train_acc - test_acc)       # Si es alto ‚Üí mucha varianza\n",
        "\n",
        "    # Guardar m√©tricas\n",
        "    resultados[\"model\"].append(nombre)\n",
        "    resultados[\"train_accuracy\"].append(train_acc)\n",
        "    resultados[\"train_precision\"].append(train_prec)\n",
        "    resultados[\"train_recall\"].append(train_rec)\n",
        "    resultados[\"train_auc\"].append(train_auc)\n",
        "    resultados[\"test_accuracy\"].append(test_acc)\n",
        "    resultados[\"test_precision\"].append(test_prec)\n",
        "    resultados[\"test_recall\"].append(test_rec)\n",
        "    resultados[\"test_auc\"].append(test_auc)\n",
        "    resultados[\"bias_gap\"].append(bias_gap)\n",
        "    resultados[\"variance_gap\"].append(variance_gap)\n",
        "\n",
        "# Convertir a DataFrame\n",
        "resultados_df = pd.DataFrame(resultados).round(4)\n",
        "\n",
        "# Ordenar por ROC-AUC de test\n",
        "resultados_df = resultados_df.sort_values(\"test_auc\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(resultados_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Probabilidades de la clase positiva (PHA = 1)\n",
        "y_scores = modelamiento.predict_proba(X_test)[:, 1]\n",
        "\n",
        " #Calcular puntos ROC y el AUC\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.plot(fpr, tpr, linewidth=2, label=f\"AUC = {roc_auc:.3f}\")\n",
        "plt.plot([0,1], [0,1], 'k--', label=\"AUC = 0.5\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"Curva ROC y AUC\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3zjE5rDS6BAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ajuste de Hiperpar√°metros\n"
      ],
      "metadata": {
        "id": "uPSxPAdEtZdx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considerando los resultados anteriores, donde ambos modelos mostraron m√©tricas muy altas y una diferencia m√≠nima entre entrenamiento y prueba, nuestro objetivo fue evitar un posible sobreajuste futuro y mejorar la estabilidad del comportamiento del modelo al enfrentarse a nuevos datos y para lograrlo, ajustaremos los hiperpar√°metros.\n",
        "\n",
        "En el caso de Random Forest, esto se hizo limitando la profundidad de los √°rboles y exigiendo un mayor n√∫mero de muestras para generar divisiones, lo que ayuda a que el modelo tome decisiones m√°s generales. Adem√°s, se redujo la cantidad de variables consideradas por √°rbol y el tama√±o de la muestra usada en cada bootstrap, lo que aumenta la diversidad entre √°rboles y disminuye la varianza del ensamble.\n",
        "\n",
        "Eb Gradient Boosting buscamos un aprendizaje m√°s gradual para evitar que el modelo memorice los datos de entrenamiento y esto se logr√≥ usando una tasa de aprendizaje baja y √°rboles muy poco profundos, de modo que cada etapa contribuya de manera peque√±a. Tambi√©n se aplic√≥ un subsample menor al 100% para introducir aleatoriedad y reducir la varianza."
      ],
      "metadata": {
        "id": "IWbSP8rq_CXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelos = {\n",
        "    \"RF\": RandomForestClassifier(\n",
        "        n_estimators=120,\n",
        "        max_depth=4,\n",
        "        min_samples_split=30,\n",
        "        min_samples_leaf=15,\n",
        "        max_features=0.4,\n",
        "        bootstrap=True,\n",
        "        max_samples=0.6\n",
        "    ),\n",
        "    \"GBC\": GradientBoostingClassifier(\n",
        "        n_estimators=120,\n",
        "        learning_rate=0.02,\n",
        "        max_depth=1,\n",
        "        min_samples_split=25,\n",
        "        min_samples_leaf=12,\n",
        "        subsample=0.6\n",
        "    )\n",
        "}\n",
        "\n",
        "resultados = {\n",
        "    \"model\": [],\n",
        "    \"train_accuracy\": [], \"test_accuracy\": [],\n",
        "    \"train_precision\": [], \"test_precision\": [],\n",
        "    \"train_recall\": [], \"test_recall\": [],\n",
        "    \"train_auc\": [], \"test_auc\": [],\n",
        "    \"bias_gap\": [],       # Diagn√≥stico\n",
        "    \"variance_gap\": []    # Diagn√≥stico\n",
        "}\n",
        "\n",
        "for nombre, modelamiento in modelos.items():\n",
        "    modelamiento.fit(X_train, y_train)\n",
        "\n",
        "    y_train_pred = modelamiento.predict(X_train)\n",
        "    y_test_pred = modelamiento.predict(X_test)\n",
        "\n",
        "    y_train_prob = modelamiento.predict_proba(X_train)[:, 1]\n",
        "    y_test_prob = modelamiento.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # M√©tricas simples y cl√°sicas\n",
        "    train_acc = accuracy_score(y_train, y_train_pred)\n",
        "    test_acc = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "    train_prec = precision_score(y_train, y_train_pred)\n",
        "    test_prec = precision_score(y_test, y_test_pred)\n",
        "\n",
        "    train_rec = recall_score(y_train, y_train_pred)\n",
        "    test_rec = recall_score(y_test, y_test_pred)\n",
        "\n",
        "    train_auc = roc_auc_score(y_train, y_train_prob)\n",
        "    test_auc = roc_auc_score(y_test, y_test_prob)\n",
        "\n",
        "    # Diagn√≥stico\n",
        "    bias_gap = abs(1 - train_acc)\n",
        "    variance_gap = abs(train_acc - test_acc)\n",
        "\n",
        "\n",
        "    resultados[\"model\"].append(nombre)\n",
        "    resultados[\"train_accuracy\"].append(train_acc)\n",
        "    resultados[\"test_accuracy\"].append(test_acc)\n",
        "    resultados[\"train_precision\"].append(train_prec)\n",
        "    resultados[\"test_precision\"].append(test_prec)\n",
        "    resultados[\"train_recall\"].append(train_rec)\n",
        "    resultados[\"test_recall\"].append(test_rec)\n",
        "    resultados[\"train_auc\"].append(train_auc)\n",
        "    resultados[\"test_auc\"].append(test_auc)\n",
        "    resultados[\"bias_gap\"].append(bias_gap)\n",
        "    resultados[\"variance_gap\"].append(variance_gap)\n",
        "\n",
        "resultados_df = pd.DataFrame(resultados).round(4)\n",
        "resultados_df = resultados_df.sort_values(\"test_auc\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(resultados_df)\n"
      ],
      "metadata": {
        "id": "h7Ibd5YUvNz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "\n",
        "  model  train_accuracy  train_precision  train_recall  train_auc\n",
        "0    RF             1.0              1.0         1.000      1.000   \n",
        "1   GBC             1.0              1.0         0.988      0.988   \n",
        "\n",
        "   test_accuracy  test_precision  test_recall  test_auc  bias_gap\n",
        "0         0.9999          0.9815       0.9636       1.0       0.0   \n",
        "1         1.0000          0.9821       1.0000       1.0       0.0   \n",
        "\n",
        "   variance_gap  \n",
        "0        0.0001  \n",
        "1        0.0000  \n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "  model  train_accuracy  test_accuracy  train_precision  test_precision\n",
        "0    RF             1.0         0.9999            0.988          0.9815   \n",
        "1   GBC             1.0         1.0000            0.988          0.9821   \n",
        "\n",
        "   train_recall  test_recall  train_auc  test_auc  bias_gap  variance_gap  \n",
        "0         0.988       0.9636        1.0       1.0       0.0        0.0001  \n",
        "1         0.994       1.0000        1.0       1.0       0.0        0.0000  \n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "hKi5C2-Jx08S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con los hiperpar√°metros ajustados, ambos modelos vuelven a mostrar m√©tricas muy altas tanto en entrenamiento como en prueba, con diferencias pr√°cticamente nulas entre ambos conjuntos, lo que sugiere que al menos seg√∫n estas m√©tricas, no hay se√±ales claras de sobreajuste. Aun as√≠, el hecho de que los valores sean tan cercanos a la perfecci√≥n deja abierta la posibilidad de que el modelo est√© dependiendo de relaciones demasiado fuertes presentes en algunas variables.\n",
        "\n",
        "No se puede afirmar que no exista ning√∫n grado de overfitting, existe la posibilidad que el dataset contenga variables altamente informativas que hagan que el problema sea ‚Äúf√°cil‚Äù para los algoritmos.\n",
        "\n",
        "Realizaremosun peque√±o experimento eliminando H y moid como variables predictoras y dejando el resto intacto. Con este paso podremos verificar si el alto rendimiento proviene de una generalizaci√≥n real o si el modelo estaba apoy√°ndose demasiado en variables muy determin√≠sticas, lo que ayudar√≠a a identificar cualquier sobreajuste que no sea evidente √∫nicamente a partir de las m√©tricas iniciales."
      ],
      "metadata": {
        "id": "6HmjdaHgxzK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = datos_n_clasificacion.drop(columns=[\"pha\",\"H\",\"moid\",\"class\"])\n",
        "y = datos_n_clasificacion[\"pha\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "modelos = {\n",
        "    \"RF\": RandomForestClassifier(\n",
        "        n_estimators=120,\n",
        "        max_depth=4,\n",
        "        min_samples_split=30,\n",
        "        min_samples_leaf=15,\n",
        "        max_features=0.4,\n",
        "        bootstrap=True,\n",
        "        max_samples=0.6\n",
        "    ),\n",
        "    \"GBC\": GradientBoostingClassifier(\n",
        "        n_estimators=120,\n",
        "        learning_rate=0.02,\n",
        "        max_depth=1,\n",
        "        min_samples_split=25,\n",
        "        min_samples_leaf=12,\n",
        "        subsample=0.6\n",
        "    )\n",
        "}\n",
        "\n",
        "resultados = {\n",
        "    \"model\": [],\n",
        "    \"train_accuracy\": [], \"test_accuracy\": [],\n",
        "    \"train_precision\": [], \"test_precision\": [],\n",
        "    \"train_recall\": [], \"test_recall\": [],\n",
        "    \"train_auc\": [], \"test_auc\": [],\n",
        "    \"bias_gap\": [],       # Diagn√≥stico\n",
        "    \"variance_gap\": []    # Diagn√≥stico\n",
        "}\n",
        "\n",
        "for nombre, modelamiento in modelos.items():\n",
        "    modelamiento.fit(X_train, y_train)\n",
        "\n",
        "    y_train_pred = modelamiento.predict(X_train)\n",
        "    y_test_pred = modelamiento.predict(X_test)\n",
        "\n",
        "    y_train_prob = modelamiento.predict_proba(X_train)[:, 1]\n",
        "    y_test_prob = modelamiento.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # M√©tricas simples y cl√°sicas\n",
        "    train_acc = accuracy_score(y_train, y_train_pred)\n",
        "    test_acc = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "    train_prec = precision_score(y_train, y_train_pred)\n",
        "    test_prec = precision_score(y_test, y_test_pred)\n",
        "\n",
        "    train_rec = recall_score(y_train, y_train_pred)\n",
        "    test_rec = recall_score(y_test, y_test_pred)\n",
        "\n",
        "    train_auc = roc_auc_score(y_train, y_train_prob)\n",
        "    test_auc = roc_auc_score(y_test, y_test_prob)\n",
        "\n",
        "    # Diagn√≥stico\n",
        "    bias_gap = abs(1 - train_acc)\n",
        "    variance_gap = abs(train_acc - test_acc)\n",
        "\n",
        "    # Guardar\n",
        "    resultados[\"model\"].append(nombre)\n",
        "    resultados[\"train_accuracy\"].append(train_acc)\n",
        "    resultados[\"test_accuracy\"].append(test_acc)\n",
        "    resultados[\"train_precision\"].append(train_prec)\n",
        "    resultados[\"test_precision\"].append(test_prec)\n",
        "    resultados[\"train_recall\"].append(train_rec)\n",
        "    resultados[\"test_recall\"].append(test_rec)\n",
        "    resultados[\"train_auc\"].append(train_auc)\n",
        "    resultados[\"test_auc\"].append(test_auc)\n",
        "    resultados[\"bias_gap\"].append(bias_gap)\n",
        "    resultados[\"variance_gap\"].append(variance_gap)\n",
        "\n",
        "resultados_df = pd.DataFrame(resultados).round(4)\n",
        "resultados_df = resultados_df.sort_values(\"test_auc\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(resultados_df)"
      ],
      "metadata": {
        "id": "INPDsmSwy4oU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al eliminar las variables h y moid, ambos modelos pierden gran parte de su capacidad para identificar la clase positiva por ejemplo Random Forest a√∫n logra predecir algunos casos PHA = 1, pero con un recall muy bajo tanto en entrenamiento como en prueba, lo que indica que ahora el modelo es extremadamente conservador y solo marca como positivos aquellos casos en los que est√° completamente seguro; esto explica que su precisi√≥n en test llegue a 1.0, pero a costa de detectar solo una peque√±a fracci√≥n de los verdaderos positivos y Gradient Boosting queda completamente ciego: no predice ning√∫n positivo, lo que hace que su precisi√≥n y su recall sean 0 y genera las advertencias de sklearn sobre m√©tricas indefinidas.\n",
        "\n",
        "\n",
        "```\n",
        "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
        "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
        "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
        "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
        "```\n",
        "\n",
        "\n",
        "A pesar de esto, ambos modelos mantienen un AUC cercano a 1.0, lo que significa que a√∫n consiguen ordenar correctamente las observaciones por probabilidad, pero el umbral por defecto no es suficiente para que crucen hacia la clase positiva. En conclusi√≥n, estos resultados muestran que h y moid eran variables altamente informativas y, sin ellas, los modelos pierden la estructura principal que les permit√≠a distinguir objetos potencialmente peligrosos.\n"
      ],
      "metadata": {
        "id": "k7Km0ixMCrWH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusiones y Recomendaciones\n",
        "\n",
        "Considerando el objetivo y los resultados mostrados, nuestras conclusiones y recomendaciones son:\n",
        "\n",
        "Con los hiperpar√°metros por defecto ambos modelos muestran desempe√±o casi perfecto:\n",
        "\n",
        "* Random Forest:\n",
        "\n",
        "```\n",
        " train_acc=1.0, test_acc=0.9999, train_prec=1.0, test_prec=0.9815, train_rec=1.0, test_rec=0.9636, train_auc=1.0, test_auc=1.0\n",
        "```\n",
        "\n",
        "\n",
        "* Gradient Boosting:\n",
        "\n",
        "```\n",
        "train_acc=1.0, test_acc=1.0, train_prec=1.0, test_prec=0.9821, train_rec=0.988, test_rec=1.0, train_auc=0.988, test_auc=1.0\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "Al aplicar un ajuste de valor a los hiperpar√°metro de los modelos se vuelven un poco menos ‚Äúperfectos‚Äù en entrenamiento pero mantienen un rendimiento de prueba igualmente elevado:\n",
        "\n",
        "* Random Forest: train_prec=0.988, test_prec=0.9815, train_rec=0.988, test_rec=0.9636, train_auc=1.0, test_auc=1.0.\n",
        "* Gradient Boosting: train_prec=0.988, test_prec=0.9821, train_rec=0.994, test_rec=1.0, train_auc=1.0, test_auc=1.0\n",
        "\n",
        "Ventajas observadas:\n",
        "\n",
        "* Ambos modelos logran ordenar y separar muy bien las clases (AUC = 1.0 en test), lo que indica que las se√±ales para distinguir PHA existen y son fuertes.\n",
        "* El ajuste de los hiperpar√°metros redujo ligeramente las m√©tricas de entrenamiento pero no sacrific√≥ el desempe√±o en test, es decir, aument√≥ la robustez sin perder poder predictivo.\n",
        "\n",
        "Limitaciones y riesgos:\n",
        "\n",
        "* Como ya mencionanmos las m√©tricas casi perfectas en entrenamiento y prueba alarman sobre posible data leakage o variables dominantes. De hecho, el experimento de eliminar h y moid demostr√≥ que esas variables aportaban la mayor parte de la se√±al: sin ellas el GBC dej√≥ de predecir positivos y el RF perdi√≥ mucho recall. Esto sugiere dependencia fuerte de esas features (posible ‚Äúatajo‚Äù hacia la etiqueta).\n",
        "* Para la misi√≥n de detectar PHA el coste de un falso negativo es alto. Aunque la precisi√≥n es alta, el recall en algunos casos baja (RF test_recall = 0.9636), y cualquier ca√≠da en recall puede ser cr√≠tica.\n",
        "* AUC alta + recall/precision extremos pueden ocultar problemas si la evaluaci√≥n no es suficientemente robusta.\n",
        "\n",
        "## Recomendaciones pr√°cticas (priorizadas)\n",
        "\n",
        "1. Investigar posible dependencia fuerte:\n",
        "\n",
        "   * Revisa correlaciones, verifica que h y moid no est√©n calculadas usando informaci√≥n futura o la etiqueta.\n",
        "   * Repetir experimentos dejando fuera h y moid y comparar ca√≠da en recall y AUC.\n",
        "\n",
        "2. Prueba en datos ‚Äúfuera de muestra‚Äù: correr modelos en conjuntos independientes para validar generalizaci√≥n, ya que en astrof√≠sica, el dataset puede cambiar entre campa√±as instrumentales.\n",
        "\n",
        "## Resumen comparativo final\n",
        "\n",
        "El Random Forest, obtiene un rendimiento muy alto (test_acc = 0.9999, test_rec = 0.9636), pero muestra indicios de sobreajuste porque es perfecto en entrenamiento y pierde recall en prueba. El Gradient Boosting alcanza un desempe√±o igualmente alto, pero con una ventaja clave: test_recall = 1.0, es decir, identifica todos los PHA en el set de prueba. Con ajuste de hiperpar√°metros, ambos modelos mantienen test_auc = 1.0 y precisiones similares (0.981‚Äì0.982), pero el ajuste mejora la estabilidad entre train y test sin alterar sustancialmente la calidad predictiva.\n",
        "\n",
        "En conclusi√≥n:\n",
        "\n",
        "Para nuestro objetivo principal, el modelo m√°s adecuado es Gradient Boosting Classifier, porque logra mayor recall que RF.\n",
        "\n",
        "Su desempe√±o en test es m√°s alineado con un uso operativo donde no se puede fallar detectando un PHA.\n",
        "\n",
        "El RF tambi√©n es muy bueno, pero su recall m√°s bajo implica que podr√≠a dejar pasar objetos verdaderamente peligrosos, lo cual es menos deseable en un escenario de riesgo astron√≥mico."
      ],
      "metadata": {
        "id": "IrWj6BWO-2PM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "#**FIN DEL PROYECTO** ‚ö†Ô∏è Asteroides: ¬øPeligroso o No?\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "OZMmGJPQIg8g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uso de IA Generativa"
      ],
      "metadata": {
        "id": "p4XkccVsMUhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* quiero categorizar las columas me puedes ayudar?\n",
        "\n",
        "\n",
        " Calcular porcentajes de valores faltantes\n",
        "porcentajes_faltantes = datos.isnull().mean() * 100\n",
        " Crear los rangos vac√≠os\n",
        "rango = {\n",
        "    \"0-10%\": [],\n",
        "    \"10-20%\": [],\n",
        "    \"20-30%\": [],\n",
        "    \"30-40%\": [],\n",
        "    \"50-60%\": [],\n",
        "    \"60-70%\": [],\n",
        "    \"70-80%\": [],\n",
        "    \"80-90%\": [],\n",
        "    \"90-100%\": []\n",
        "}\n",
        "**Clasificar cada columna seg√∫n su porcentaje de NA**\n",
        "for col, pct in porcentajes_faltantes.items():\n",
        "    if pct <= 10:\n",
        "        rango[\"0-10%\"].append(col)\n",
        "    elif pct <= 20:\n",
        "        rango[\"10-20%\"].append(col)\n",
        "    elif pct <= 30:\n",
        "        rango[\"20-30%\"].append(col)\n",
        "    elif pct <= 40:\n",
        "        rango[\"30-40%\"].append(col)\n",
        "    elif pct <= 50:\n",
        "        rango[\"40-50%\"].append(col)\n",
        "    elif pct <= 60:\n",
        "        rango[\"50-60%\"].append(col)\n",
        "    elif pct <= 70:\n",
        "        rango[\"60-70%\"].append(col)\n",
        "    elif pct <= 80:\n",
        "        rango[\"70-80%\"].append(col)\n",
        "    elif pct <= 90:\n",
        "        rango[\"80-90%\"].append(col)\n",
        "    else:\n",
        "        rango[\"90-100%\"].append(col)\n",
        "**Mostrar resultados**\n",
        "rango\n"
      ],
      "metadata": {
        "id": "zwB7-tQyMpGT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4Hn3XOD2O-_N"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}